{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a7ebc0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513           neutral                        1.0000   \n",
       "1  570301130888122368          positive                        0.3486   \n",
       "2  570301083672813571           neutral                        0.6837   \n",
       "3  570301031407624196          negative                        1.0000   \n",
       "4  570300817074462722          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                     0.0000  Virgin America   \n",
       "2            NaN                        NaN  Virgin America   \n",
       "3     Bad Flight                     0.7033  Virgin America   \n",
       "4     Can't Tell                     1.0000  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
       "0                    NaN     cairdin                 NaN              0   \n",
       "1                    NaN    jnardino                 NaN              0   \n",
       "2                    NaN  yvonnalynn                 NaN              0   \n",
       "3                    NaN    jnardino                 NaN              0   \n",
       "4                    NaN    jnardino                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "\n",
       "               tweet_created tweet_location               user_timezone  \n",
       "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
       "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
       "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
       "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
       "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Definimos la ruta a nuestro archivo de datos\n",
    "data_path = \"../data/raw/Tweets.csv\"\n",
    "\n",
    "# Cargamos los datos en un DataFrame de pandas\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Â¡Mostramos las primeras 5 filas para ver cÃ³mo se ven los datos!\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "147b54e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistribuciÃ³n de sentimientos:\n",
      "airline_sentiment\n",
      "negative    9178\n",
      "neutral     3099\n",
      "positive    2363\n",
      "Name: count, dtype: int64\n",
      "\n",
      "==============================\n",
      "\n",
      "Datos limpios:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>airline_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text airline_sentiment\n",
       "0                @VirginAmerica What @dhepburn said.           neutral\n",
       "1  @VirginAmerica plus you've added commercials t...          positive\n",
       "2  @VirginAmerica I didn't today... Must mean I n...           neutral\n",
       "3  @VirginAmerica it's really aggressive to blast...          negative\n",
       "4  @VirginAmerica and it's a really big bad thing...          negative"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- 1. Ver la distribuciÃ³n de los datos ---\n",
    "# Queremos saber si los datos estÃ¡n balanceados (equilibrados)\n",
    "print(\"DistribuciÃ³n de sentimientos:\")\n",
    "print(df['airline_sentiment'].value_counts())\n",
    "print(\"\\n\" + \"=\"*30 + \"\\n\")\n",
    "\n",
    "\n",
    "# --- 2. Crear un DataFrame limpio ---\n",
    "# Seleccionamos solo las columnas que realmente usaremos\n",
    "df_limpio = df[['text', 'airline_sentiment']]\n",
    "\n",
    "# Mostramos las primeras 5 filas del nuevo DataFrame limpio\n",
    "print(\"Datos limpios:\")\n",
    "df_limpio.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a00ec035",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25106/2184892709.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_limpio['sentiment_encoded'] = encoder.fit_transform(df_limpio['airline_sentiment'])\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "/tmp/ipykernel_25106/2184892709.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_limpio['text_limpio'] = df_limpio['text'].apply(limpiar_texto)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Etiquetas codificadas:\n",
      "  airline_sentiment  sentiment_encoded\n",
      "0           neutral                  1\n",
      "1          positive                  2\n",
      "2           neutral                  1\n",
      "3          negative                  0\n",
      "4          negative                  0\n",
      "\n",
      "==============================\n",
      "\n",
      "Limpiando el texto...\n",
      "Datos finales listos para el modelo:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_limpio</th>\n",
       "      <th>sentiment_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>said</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>plus youve added commercials experience tacky</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>didnt today must mean need take another trip</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>really aggressive blast obnoxious entertainmen...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>really big bad thing</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0                @VirginAmerica What @dhepburn said.   \n",
       "1  @VirginAmerica plus you've added commercials t...   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...   \n",
       "3  @VirginAmerica it's really aggressive to blast...   \n",
       "4  @VirginAmerica and it's a really big bad thing...   \n",
       "\n",
       "                                         text_limpio  sentiment_encoded  \n",
       "0                                               said                  1  \n",
       "1      plus youve added commercials experience tacky                  2  \n",
       "2       didnt today must mean need take another trip                  1  \n",
       "3  really aggressive blast obnoxious entertainmen...                  0  \n",
       "4                               really big bad thing                  0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# --- 1. Codificar las Etiquetas (Labels) ---\n",
    "\n",
    "# Usaremos un LabelEncoder para convertir el texto a nÃºmeros\n",
    "# negative -> 0, neutral -> 1, positive -> 2\n",
    "encoder = LabelEncoder()\n",
    "df_limpio['sentiment_encoded'] = encoder.fit_transform(df_limpio['airline_sentiment'])\n",
    "\n",
    "# Vemos el resultado\n",
    "print(\"Etiquetas codificadas:\")\n",
    "print(df_limpio[['airline_sentiment', 'sentiment_encoded']].head())\n",
    "print(\"\\n\" + \"=\"*30 + \"\\n\")\n",
    "\n",
    "\n",
    "# --- 2. Limpieza de Texto (Text Cleaning) ---\n",
    "\n",
    "# Descargamos la lista de \"stopwords\" (palabras comunes como 'y', 'el', 'la', 'a')\n",
    "# Esto solo se hace una vez\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def limpiar_texto(texto):\n",
    "    # Convertir a minÃºsculas\n",
    "    texto = texto.lower()\n",
    "    # Quitar @menciones\n",
    "    texto = re.sub(r'@\\w+', '', texto)\n",
    "    # Quitar puntuaciÃ³n y nÃºmeros (dejar solo letras)\n",
    "    texto = re.sub(r'[^a-zA-Z\\s]', '', texto)\n",
    "    # Quitar stopwords\n",
    "    texto = ' '.join(palabra for palabra in texto.split() if palabra not in stop_words)\n",
    "    return texto\n",
    "\n",
    "# Aplicamos la funciÃ³n de limpieza a toda la columna 'text'\n",
    "print(\"Limpiando el texto...\")\n",
    "df_limpio['text_limpio'] = df_limpio['text'].apply(limpiar_texto)\n",
    "\n",
    "# Mostramos el resultado final\n",
    "print(\"Datos finales listos para el modelo:\")\n",
    "df_limpio[['text', 'text_limpio', 'sentiment_encoded']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0a7f8a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-17 01:29:16.314567: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-11-17 01:29:17.895600: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-17 01:29:20.506367: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forma de los datos de X (secuencias): (14640, 50)\n",
      "Ejemplo de secuencia rellenada:\n",
      "[ 418  419  985 2219  112 5676    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0]\n",
      "\n",
      "==============================\n",
      "\n",
      "Forma de los datos de Y (etiquetas): (14640, 3)\n",
      "Ejemplo de etiqueta 'one-hot' (era 'positive' -> 2):\n",
      "[0. 0. 1.]\n",
      "\n",
      "==============================\n",
      "\n",
      "Datos listos:\n",
      "X_train (entrenamiento): (11712, 50)\n",
      "y_train (entrenamiento): (11712, 3)\n",
      "X_test (prueba): (2928, 50)\n",
      "y_test (prueba): (2928, 3)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Definir ParÃ¡metros ---\n",
    "MAX_VOCAB_SIZE = 10000  # CuÃ¡ntas palabras Ãºnicas recordarÃ¡ (las 10,000 mÃ¡s comunes)\n",
    "MAX_SEQ_LENGTH = 50     # El largo mÃ¡ximo de un tuit (en palabras)\n",
    "\n",
    "# --- 2. TokenizaciÃ³n ---\n",
    "# Inicializa el Tokenizer\n",
    "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE)\n",
    "\n",
    "# \"Aprende\" el vocabulario de nuestros textos limpios\n",
    "tokenizer.fit_on_texts(df_limpio['text_limpio'])\n",
    "\n",
    "# Convierte los textos en secuencias de nÃºmeros\n",
    "X = tokenizer.texts_to_sequences(df_limpio['text_limpio'])\n",
    "\n",
    "# --- 3. Padding (Relleno) ---\n",
    "# Rellena (o corta) las secuencias para que TODAS midan MAX_SEQ_LENGTH\n",
    "X_pad = pad_sequences(X, maxlen=MAX_SEQ_LENGTH, padding='post', truncating='post')\n",
    "\n",
    "print(f\"Forma de los datos de X (secuencias): {X_pad.shape}\")\n",
    "print(\"Ejemplo de secuencia rellenada:\")\n",
    "print(X_pad[1]) # Muestra el segundo tuit (\"plus youve added...\") en nÃºmeros\n",
    "\n",
    "print(\"\\n\" + \"=\"*30 + \"\\n\")\n",
    "\n",
    "# --- 4. Preparar las Etiquetas (Y) ---\n",
    "# Necesitamos convertir nuestras etiquetas (0, 1, 2) al formato \"one-hot\"\n",
    "# (0 -> [1,0,0], 1 -> [0,1,0], 2 -> [0,0,1])\n",
    "y = to_categorical(df_limpio['sentiment_encoded'])\n",
    "\n",
    "print(f\"Forma de los datos de Y (etiquetas): {y.shape}\")\n",
    "print(\"Ejemplo de etiqueta 'one-hot' (era 'positive' -> 2):\")\n",
    "print(y[1]) # Muestra la etiqueta del segundo tuit\n",
    "\n",
    "print(\"\\n\" + \"=\"*30 + \"\\n\")\n",
    "\n",
    "# --- 5. DivisiÃ³n Train/Test ---\n",
    "# Dividimos el 80% para entrenar y 20% para probar\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_pad, \n",
    "    y, \n",
    "    test_size=0.2, \n",
    "    random_state=42,  # Para que la divisiÃ³n sea siempre igual\n",
    "    stratify=y        # Asegura que tengamos % similares de 0, 1 y 2 en ambos sets\n",
    ")\n",
    "\n",
    "print(\"Datos listos:\")\n",
    "print(f\"X_train (entrenamiento): {X_train.shape}\")\n",
    "print(f\"y_train (entrenamiento): {y_train.shape}\")\n",
    "print(f\"X_test (prueba): {X_test.shape}\")\n",
    "print(f\"y_test (prueba): {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11111ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/sentiment-analysis-api/.venv/lib/python3.11/site-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n",
      "2025-11-17 01:36:48.273361: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280,000</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">98,816</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">387</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m128\u001b[0m)        â”‚     \u001b[38;5;34m1,280,000\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚        \u001b[38;5;34m98,816\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              â”‚           \u001b[38;5;34m387\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,379,203</span> (5.26 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,379,203\u001b[0m (5.26 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,379,203</span> (5.26 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,379,203\u001b[0m (5.26 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# --- 1. Definir la Arquitectura ---\n",
    "\n",
    "# Definimos el tamaÃ±o de entrada (nuestro MAX_SEQ_LENGTH)\n",
    "input_layer = Input(shape=(MAX_SEQ_LENGTH,))\n",
    "\n",
    "# Capa de Embedding\n",
    "# (MAX_VOCAB_SIZE viene de la celda anterior, eran 10000)\n",
    "# output_dim=128 significa que cada palabra serÃ¡ un vector de 128 dimensiones\n",
    "embedding_layer = Embedding(input_dim=MAX_VOCAB_SIZE, \n",
    "                            output_dim=128, \n",
    "                            input_length=MAX_SEQ_LENGTH)(input_layer)\n",
    "\n",
    "# Capa Bidireccional LSTM con 64 unidades de memoria\n",
    "# Dropout=0.2 \"apaga\" el 20% de las neuronas al azar para evitar sobreajuste\n",
    "lstm_layer = Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2))(embedding_layer)\n",
    "\n",
    "# Capa de salida\n",
    "# 3 neuronas (una para cada clase) con activaciÃ³n 'softmax' (para probabilidades)\n",
    "output_layer = Dense(3, activation='softmax')(lstm_layer)\n",
    "\n",
    "# --- 2. Construir el Modelo ---\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# --- 3. Compilar el Modelo ---\n",
    "# 'adam' es el optimizador estÃ¡ndar\n",
    "# 'categorical_crossentropy' es la funciÃ³n de pÃ©rdida para 3+ clases\n",
    "# 'accuracy' es lo que queremos medir\n",
    "model.compile(optimizer='adam', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# --- 4. Ver el Resumen ---\n",
    "# Â¡Esto nos muestra un mapa de nuestro modelo!\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0998fe6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comenzando el entrenamiento... â˜•\n",
      "Epoch 1/10\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 97ms/step - accuracy: 0.6914 - loss: 0.7345 - val_accuracy: 0.7713 - val_loss: 0.5506\n",
      "Epoch 2/10\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 95ms/step - accuracy: 0.8294 - loss: 0.4429 - val_accuracy: 0.7841 - val_loss: 0.5217\n",
      "Epoch 3/10\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 93ms/step - accuracy: 0.8824 - loss: 0.3160 - val_accuracy: 0.7850 - val_loss: 0.5644\n",
      "Epoch 4/10\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 97ms/step - accuracy: 0.9128 - loss: 0.2382 - val_accuracy: 0.7756 - val_loss: 0.6574\n",
      "Â¡Entrenamiento finalizado! ğŸ‰\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Definir \"Early Stopping\" ---\n",
    "# Esto es un guardiÃ¡n inteligente.\n",
    "# Le decimos que \"vigile\" la pÃ©rdida en los datos de prueba (val_loss).\n",
    "# Si el 'val_loss' no mejora durante 2 Ã©pocas seguidas,\n",
    "# detendrÃ¡ el entrenamiento automÃ¡ticamente.\n",
    "# Esto nos ahorra tiempo y evita que el modelo \"sobreaprenda\".\n",
    "early_stop = EarlyStopping(monitor='val_loss', \n",
    "                             patience=2, \n",
    "                             restore_best_weights=True)\n",
    "\n",
    "# --- 2. Â¡Comenzar el Entrenamiento! ---\n",
    "print(\"Comenzando el entrenamiento... â˜•\")\n",
    "\n",
    "# AquÃ­ le pasamos los datos de entrenamiento (X_train, y_train)\n",
    "# Le decimos que use el 10% de esos datos para validaciÃ³n (validation_split=0.1)\n",
    "# Le pedimos que entrene por un mÃ¡ximo de 10 Ã©pocas (epochs=10)\n",
    "# Le decimos que use nuestro guardiÃ¡n (callbacks=[early_stop])\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=10,\n",
    "    batch_size=64,\n",
    "    validation_split=0.1,  # Usa el 10% para validar en cada Ã©poca\n",
    "    callbacks=[early_stop]\n",
    ")\n",
    "\n",
    "print(\"Â¡Entrenamiento finalizado! ğŸ‰\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2aa51746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluando el modelo en el set de prueba...\n",
      "\u001b[1m92/92\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.7954 - loss: 0.5235\n",
      "PÃ©rdida en el Test Set: 0.5235\n",
      "PrecisiÃ³n en el Test Set: 79.54%\n",
      "\n",
      "==============================\n",
      "\n",
      "\u001b[1m92/92\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step\n",
      "Reporte de ClasificaciÃ³n:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.92      0.88      1835\n",
      "     neutral       0.66      0.51      0.57       620\n",
      "    positive       0.76      0.69      0.72       473\n",
      "\n",
      "    accuracy                           0.80      2928\n",
      "   macro avg       0.75      0.71      0.72      2928\n",
      "weighted avg       0.79      0.80      0.79      2928\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Evaluar el modelo en el Test Set ---\n",
    "# Esto nos da la pÃ©rdida (loss) y la precisiÃ³n (accuracy) finales\n",
    "print(\"Evaluando el modelo en el set de prueba...\")\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"PÃ©rdida en el Test Set: {loss:.4f}\")\n",
    "print(f\"PrecisiÃ³n en el Test Set: {accuracy * 100:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*30 + \"\\n\")\n",
    "\n",
    "# --- 2. Generar Reporte de ClasificaciÃ³n ---\n",
    "# Hacemos predicciones en el set de prueba\n",
    "y_pred_probs = model.predict(X_test)\n",
    "# Convertimos las probabilidades (ej. [0.1, 0.8, 0.1]) a una sola clase (ej. 1)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "# TambiÃ©n necesitamos convertir el y_test (que es one-hot) a clases\n",
    "y_test_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Obtenemos los nombres de las etiquetas (ej. 0 -> 'negative')\n",
    "# Â¡OJO! El orden debe ser el que aprendiÃ³ el encoder.\n",
    "# Si 0=negative, 1=neutral, 2=positive...\n",
    "target_names = encoder.classes_\n",
    "\n",
    "# Â¡Imprimimos el reporte!\n",
    "print(\"Reporte de ClasificaciÃ³n:\")\n",
    "print(classification_report(y_test_labels, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a6c8272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardando el modelo en ../src/model/sentiment_model.keras...\n",
      "Guardando el tokenizer en ../src/model/tokenizer.json...\n",
      "Â¡Modelo y tokenizer guardados con Ã©xito! ğŸ’¾\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# --- 1. Definir las rutas donde guardaremos ---\n",
    "model_path = \"../src/model/sentiment_model.keras\"\n",
    "tokenizer_path = \"../src/model/tokenizer.json\"\n",
    "\n",
    "# --- 2. Guardar el Modelo de Keras ---\n",
    "print(f\"Guardando el modelo en {model_path}...\")\n",
    "model.save(model_path)\n",
    "\n",
    "# --- 3. Guardar el Tokenizer ---\n",
    "# El tokenizer no se guarda con model.save(), hay que hacerlo aparte\n",
    "print(f\"Guardando el tokenizer en {tokenizer_path}...\")\n",
    "tokenizer_config = tokenizer.to_json()\n",
    "with open(tokenizer_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(tokenizer_config, ensure_ascii=False))\n",
    "\n",
    "print(\"Â¡Modelo y tokenizer guardados con Ã©xito! ğŸ’¾\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb8e6198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardando el modelo en ../src/model/sentiment_model.keras...\n",
      "Guardando el tokenizer en ../src/model/tokenizer.json...\n",
      "Â¡Modelo y tokenizer guardados con Ã©xito! ğŸ’¾\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# --- 1. Definir las rutas donde guardaremos ---\n",
    "model_path = \"../src/model/sentiment_model.keras\"\n",
    "tokenizer_path = \"../src/model/tokenizer.json\"\n",
    "\n",
    "# --- 2. Guardar el Modelo de Keras ---\n",
    "print(f\"Guardando el modelo en {model_path}...\")\n",
    "model.save(model_path)\n",
    "\n",
    "# --- 3. Guardar el Tokenizer ---\n",
    "# El tokenizer no se guarda con model.save(), hay que hacerlo aparte\n",
    "print(f\"Guardando el tokenizer en {tokenizer_path}...\")\n",
    "tokenizer_config = tokenizer.to_json()\n",
    "with open(tokenizer_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(tokenizer_config, ensure_ascii=False))\n",
    "\n",
    "print(\"Â¡Modelo y tokenizer guardados con Ã©xito! ğŸ’¾\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
